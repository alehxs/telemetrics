# F1 Race Prediction ML Pipeline

## ğŸ¯ Overview

This ML pipeline predicts F1 race results using **Gradient Boosting Regression**, trained on historical race data (2018-2024). Your task is to implement the core ML components as a hands-on learning exercise.

## ğŸ“ Project Structure

```
backend/scripts/ml_models/
â”œâ”€â”€ ml_config.py              âœ“ [Complete] Configuration constants
â”œâ”€â”€ data_loader.py            âœ“ [Complete] Load historical data from Supabase
â”œâ”€â”€ feature_engineering.py    âš ï¸ [TODO(human)] Extract ML features
â”œâ”€â”€ model_trainer.py          âš ï¸ [TODO(human)] Train Gradient Boosting
â”œâ”€â”€ predictor.py              âš ï¸ [TODO(human)] Generate predictions
â”œâ”€â”€ evaluator.py              âš ï¸ [TODO(human)] Evaluate model accuracy
â””â”€â”€ prediction_uploader.py    âœ“ [Complete] Upload to database

backend/scripts/
â””â”€â”€ predict_race.py           âœ“ [Complete] CLI orchestrator

backend/
â”œâ”€â”€ setup_predictions_table.sql âœ“ [Complete] Database schema
â””â”€â”€ requirements.txt            âœ“ [Complete] Python dependencies

frontend/src/
â”œâ”€â”€ types/predictions.ts        âœ“ [Complete] TypeScript interfaces
â”œâ”€â”€ services/predictionsService.ts âœ“ [Complete] API service
â””â”€â”€ components/RacePredictions.tsx âœ“ [Complete] Display component
```

## ğŸš€ Quick Start

### 1. Set Up Database

```bash
# Run the SQL schema to create race_predictions table
cd backend
psql <your-supabase-connection-string> < setup_predictions_table.sql

# Or use Supabase Dashboard:
# â†’ SQL Editor â†’ Copy contents of setup_predictions_table.sql â†’ Run
```

### 2. Install Dependencies

```bash
cd backend
pip install -r requirements.txt
```

### 3. Test Infrastructure (Before ML Implementation)

```bash
cd backend/scripts

# Test data loading
python -c "from ml_models.data_loader import F1DataLoader; \\
           loader = F1DataLoader(); \\
           data = loader.load_historical_races(2023, 2024); \\
           print(f'Loaded {len(data[\"merged\"])} records')"

# Test ML config
python ml_models/ml_config.py

# Test prediction uploader (with mock data)
python ml_models/prediction_uploader.py
```

## ğŸ“ Your ML Implementation Tasks

You'll implement **4 core files** in order:

### Task 1: Feature Engineering (`feature_engineering.py`)

**What to implement:**
- `FeatureEngineer.extract_features()` - Extract features for a single race
- `FeatureEngineer.extract_all_training_features()` - Extract for all historical races

**Start simple:**
```python
# Step 1: Filter data for the target race
race_data = self.merged[
    (self.merged['year'] == year) &
    (self.merged['grand_prix'] == grand_prix)
].copy()

# Step 2: Select feature columns
features_df = race_data[['driver_abbr', 'driver_name', 'team', 'qualifying_time']].copy()

# Step 3: Add target variable
if for_training:
    features_df['race_time_seconds'] = race_data['race_time_seconds']
    features_df = features_df.dropna(subset=['race_time_seconds'])

return features_df
```

**Success criteria:**
- Returns DataFrame with driver context + qualifying_time feature
- Includes race_time_seconds target when for_training=True
- No missing values in critical columns

### Task 2: Model Training (`model_trainer.py`)

**What to implement:**
- `RacePredictor.train()` - Train Gradient Boosting model
- `RacePredictor.predict()` - Generate predictions from features
- `RacePredictor.save_model()` / `load_model()` - Model persistence

**Core training loop:**
```python
# 1. Prepare data
X = training_data[FEATURE_COLUMNS]
y = training_data[TARGET_COLUMN]

# 2. Train/test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE
)

# 3. Initialize and train
self.model = GradientBoostingRegressor(**GRADIENT_BOOSTING_PARAMS)
self.model.fit(X_train, y_train)

# 4. Evaluate
y_pred = self.model.predict(X_test)
mae = mean_absolute_error(y_test, y_pred)

# 5. Extract feature importances
self.feature_importances = dict(zip(FEATURE_COLUMNS, self.model.feature_importances_))
```

**Success criteria:**
- MAE < 20 seconds on test set
- Model trains without errors
- Feature importances extracted
- Model can be saved and loaded

### Task 3: Prediction Generation (`predictor.py`)

**What to implement:**
- `PredictionGenerator.predict_race()` - Generate predictions for a race
- `PredictionGenerator.format_for_database()` - Format for DB storage

**Prediction flow:**
```python
# 1. Extract features from qualifying data
features_df = qualifying_data[['driver_abbr', 'driver_name', 'team', 'qualifying_time']].copy()

# 2. Generate predictions
predicted_times = self.model.predict(features_df)
features_df['predicted_race_time_seconds'] = predicted_times

# 3. Rank by time
features_df = features_df.sort_values('predicted_race_time_seconds')
features_df['position'] = range(1, len(features_df) + 1)

# 4. Format output
predictions = features_df.to_dict('records')
return predictions
```

**Success criteria:**
- Predictions ranked correctly (lowest time = P1)
- All drivers have valid predictions
- Output format matches database schema

### Task 4: Model Evaluation (`evaluator.py`)

**What to implement:**
- `ModelEvaluator.evaluate_predictions()` - Compare predictions vs actual results

**Evaluation metrics:**
```python
# 1. Merge predictions with actual results
merged = pred_df.merge(actual_results, on='driver_abbr')

# 2. Calculate MAE (time-based)
mae = mean_absolute_error(merged['race_time_seconds'], merged['predicted_race_time_seconds'])

# 3. Calculate position accuracy
position_diff = abs(merged['predicted_position'] - merged['actual_position'])
position_accuracy = (position_diff <= 3).sum() / len(merged)

# 4. Check podium accuracy
predicted_podium = predictions[:3]
actual_podium = actual_results.nsmallest(3, 'position')['driver_abbr'].tolist()
podium_matches = len(set(predicted_podium) & set(actual_podium))
```

**Success criteria:**
- MAE calculation works
- Position accuracy calculated (% within Â±3 positions)
- Podium accuracy calculated (how many of top 3 correct)

## ğŸ”„ Complete Workflow

Once you've implemented all 4 tasks:

```bash
# 1. Train the model
python predict_race.py train --save-model --cross-validate

# Expected output:
# âœ“ Loaded 420 historical race records
# âœ“ Extracted features for 420 samples
# âœ“ Training complete
#   Test MAE: 15.23 seconds
#   RÂ² Score: 0.812
#   Feature Importances:
#     qualifying_time: 0.850

# 2. Generate predictions for 2025 Bahrain GP
python predict_race.py predict --year 2025 --gp "Bahrain Grand Prix" --upload

# Expected output:
# âœ“ Model loaded (MAE: 15.23s)
# âœ“ Loaded qualifying data for 20 drivers
# âœ“ Generated predictions
#   Predicted winner: VER (5234.56s)
# âœ“ Uploaded to database

# 3. Evaluate on 2024 races (backtesting)
python predict_race.py evaluate --year 2024

# Expected output:
# âœ“ Evaluated 24 races
#   Season MAE: 16.78s
#   Position Accuracy: 67.3%
#   Podium Accuracy: 71.2%
#   Winner Accuracy: 54.2%
```

## ğŸ“Š Frontend Integration

Once predictions are uploaded, they'll automatically appear in the frontend:

```tsx
// The RacePredictions component will fetch and display:
// - Predicted podium with driver photos
// - Full race order table
// - Model metadata (MAE, feature importance)
// - Time gaps between drivers
```

**To view predictions:**
1. Navigate to a 2025 race in the frontend
2. The `RacePredictions` component will auto-fetch from database
3. If no predictions exist, it shows "No predictions available"

## ğŸ¯ Learning Objectives

By implementing this pipeline, you'll master:

1. **Feature Engineering**: Transforming raw F1 data into ML features
2. **Gradient Boosting**: Hyperparameter tuning, ensemble methods
3. **Train/Test Splits**: Temporal vs random splits for time-series
4. **Model Evaluation**: Domain-specific metrics (position accuracy, podium)
5. **Production ML**: Model serialization, versioning, deployment

## ğŸ› Common Issues & Solutions

### Issue: "No training data available"
**Solution:** Verify Supabase connection and data exists:
```python
from ml_models.data_loader import F1DataLoader
loader = F1DataLoader()
data = loader.load_historical_races(2018, 2024)
print(f"Merged: {len(data['merged'])} records")
```

### Issue: "Feature extraction not yet implemented"
**Solution:** This is expected! Implement `feature_engineering.py` first.

### Issue: MAE is very high (> 50 seconds)
**Possible causes:**
- Missing data (drivers with NaN qualifying times)
- Feature scaling needed (try StandardScaler)
- Overfitting (reduce max_depth, increase min_samples_split)

### Issue: Predictions all the same
**Solution:** Check that features have variance:
```python
print(features_df['qualifying_time'].describe())
# Should show different values, not all the same
```

## ğŸ“š Resources

- **Scikit-learn Gradient Boosting**: https://scikit-learn.org/stable/modules/ensemble.html#gradient-boosting
- **Feature Engineering**: https://scikit-learn.org/stable/modules/preprocessing.html
- **Model Evaluation**: https://scikit-learn.org/stable/modules/model_evaluation.html
- **Inspiration Repo**: https://github.com/mar-antaya/2025_f1_predictions

## ğŸ‰ Success Criteria

Your implementation is complete when:

- âœ… Model trains successfully on historical data
- âœ… MAE < 20 seconds on test set
- âœ… Predictions upload to database
- âœ… Frontend displays predictions correctly
- âœ… You can explain each step in an interview

## ğŸ“ Next Steps

After completing the basic pipeline:

1. **Add more features**: Track history, team averages, weather
2. **Hyperparameter tuning**: Grid search for optimal parameters
3. **Ensemble methods**: Combine multiple models (XGBoost, Random Forest)
4. **Real-time updates**: Fetch qualifying data from FastF1 API
5. **Confidence intervals**: Use quantile regression for uncertainty

---

**Ready to start?** Begin with Task 1: Feature Engineering! ğŸš€

Open `feature_engineering.py` and look for the `TODO(human)` comments.
